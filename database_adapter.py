import logging
import os
import time
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from config import USE_MONGODB, CACHE_EXPIRE_SECONDS

# Nouveau syst√®me de cache
from cache_system import (
    get_cached_data, set_cached_data,
    get_cached_teams, cache_teams,
    get_cached_matches, cache_matches,
    get_cached_subscription_status, cache_subscription_status,
    get_cached_referral_count, cache_referral_count
)

# Syst√®me de file d'attente pour les op√©rations de base de donn√©es
from queue_manager import queue_manager

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialisation de l'adaptateur
logger.info(f"Initialisation de l'adaptateur de base de donn√©es avec {'MongoDB' if USE_MONGODB else 'Google Sheets'}")

# Importer les modules appropri√©s en fonction de la configuration
if USE_MONGODB:
    try:
        import mongo_db as db
        logger.info("Utilisation de MongoDB comme base de donn√©es principale")
    except ImportError as e:
        logger.error(f"Module mongo_db non trouv√©, fallback sur Google Sheets: {e}")
        try:
            import database as db
            logger.warning("Fallback sur Google Sheets activ√©")
        except ImportError:
            logger.critical("ERREUR CRITIQUE: Aucun module de base de donn√©es disponible!")
            # Cr√©er un module de secours minimal pour √©viter les erreurs
            class EmergencyDB:
                @staticmethod
                async def check_user_subscription(user_id):
                    logger.error("EmergencyDB: check_user_subscription appel√©")
                    return True
                
                @staticmethod
                async def register_user(*args, **kwargs):
                    logger.error("EmergencyDB: register_user appel√©")
                    return True
                
                @staticmethod
                async def count_referrals(*args):
                    logger.error("EmergencyDB: count_referrals appel√©")
                    return 0
                
                @staticmethod
                def get_all_matches_data():
                    logger.error("EmergencyDB: get_all_matches_data appel√©")
                    return []
                
                @staticmethod
                def get_all_teams():
                    logger.error("EmergencyDB: get_all_teams appel√©")
                    return ["√âquipe 1", "√âquipe 2"]  # Valeurs minimales pour √©viter les crashs
            
            db = EmergencyDB()
            logger.critical("Module de secours minimal activ√© pour √©viter les crashs")
else:
    try:
        import database as db
        logger.info("Utilisation de Google Sheets comme base de donn√©es principale (configuration explicite)")
    except ImportError as e:
        logger.error(f"Module database non trouv√©: {e}")
        logger.error("Tentative d'utilisation de MongoDB comme solution de secours")
        try:
            import mongo_db as db
            logger.info("MongoDB utilis√© comme solution de secours")
        except ImportError:
            logger.critical("ERREUR CRITIQUE: Aucun module de base de donn√©es disponible!")
            raise RuntimeError("Aucun module de base de donn√©es disponible. Impossible de continuer.")

# File d'attente pour les utilisateurs √† traiter par lots
_users_batch_queue = []  # Liste des utilisateurs en attente d'enregistrement
_last_batch_processing = time.time()  # Heure du dernier traitement par lots

# Fonction pour obtenir une connexion √† la base de donn√©es
def get_database():
    """R√©cup√®re une connexion √† la base de donn√©es active"""
    if USE_MONGODB:
        try:
            from mongo_db import get_database as get_mongo_db
            return get_mongo_db()
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration de la base de donn√©es MongoDB: {e}")
            return None
    else:
        # Pour Google Sheets, retourner None car ce n'est pas une base de donn√©es traditionnelle
        return None

# Traitement par lots des utilisateurs
async def process_users_batch():
    """
    Traite le lot d'utilisateurs en attente.
    Version optimis√©e qui minimise les requ√™tes API.
    """
    global _users_batch_queue
    global _last_batch_processing
    
    # Copier et vider la file d'attente (pour √©viter les probl√®mes de concurrence)
    users_to_process = _users_batch_queue.copy()
    _users_batch_queue = []
    _last_batch_processing = time.time()
    
    if not users_to_process:
        logger.debug("Aucun utilisateur √† traiter par lots")
        return
    
    logger.info(f"Traitement par lots de {len(users_to_process)} utilisateurs")
    
    try:
        if USE_MONGODB:
            # Pour MongoDB, pr√©parer les op√©rations par lots
            db_instance = get_database()
            if db_instance is None:
                logger.error("Impossible de se connecter √† MongoDB pour le traitement par lots")
                return
            
            # Traiter par lots avec un seul appel √† la base de donn√©es
            async def process_batch_mongodb():
                # Cr√©er un tableau d'op√©rations pour une ex√©cution en bulk
                operations = []
                
                for user_data in users_to_process:
                    user_id = str(user_data["user_id"])
                    username = user_data["username"]
                    referrer_id = user_data.get("referrer_id")
                    current_time = datetime.now().isoformat()
                    
                    # V√©rifier si l'utilisateur existe d√©j√†
                    user = await db_instance.users.find_one({"user_id": user_id})
                    
                    if user:
                        # Mise √† jour
                        operations.append({
                            "update_one": {
                                "filter": {"user_id": user_id},
                                "update": {
                                    "$set": {
                                        "username": username,
                                        "last_activity": current_time
                                    },
                                    "$setOnInsert": {
                                        "registration_date": current_time,
                                        "referred_by": str(referrer_id) if referrer_id and referrer_id != user_id else None
                                    }
                                }
                            }
                        })
                    else:
                        # Insertion
                        operations.append({
                            "insert_one": {
                                "document": {
                                    "user_id": user_id,
                                    "username": username,
                                    "registration_date": current_time,
                                    "last_activity": current_time,
                                    "referred_by": str(referrer_id) if referrer_id and referrer_id != user_id else None
                                }
                            }
                        })
                
                # Ex√©cuter les op√©rations en une seule requ√™te
                if operations:
                    try:
                        result = await db_instance.users.bulk_write(operations)
                        logger.info(f"Traitement par lots termin√©: {result.inserted_count} ins√©r√©s, {result.modified_count} modifi√©s")
                    except Exception as e:
                        logger.error(f"Erreur lors de l'op√©ration bulk_write: {e}")
            
            # Ajouter √† la file d'attente pour ex√©cution asynchrone
            await queue_manager.add_low_priority(process_batch_mongodb)
        else:
            # Pour Google Sheets, traiter s√©quentiellement
            for user_data in users_to_process:
                await db.register_user(
                    user_data["user_id"], 
                    user_data["username"], 
                    user_data.get("referrer_id")
                )
                
        logger.info(f"Traitement par lots programm√© pour {len(users_to_process)} utilisateurs")
    except Exception as e:
        logger.error(f"Erreur lors du traitement par lots des utilisateurs: {e}")

async def add_user_to_batch_queue(user_id, username, referrer_id=None):
    """
    Ajoute un utilisateur √† la file d'attente pour traitement par lots.
    D√©clenche le traitement si n√©cessaire.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        username (str): Nom d'utilisateur Telegram
        referrer_id (int, optional): ID du parrain
        
    Returns:
        bool: True si l'ajout a r√©ussi
    """
    global _users_batch_queue
    global _last_batch_processing
    
    try:
        # V√©rifier si l'utilisateur est d√©j√† dans la file d'attente
        for user in _users_batch_queue:
            if user["user_id"] == user_id:
                return True
        
        # Ajouter l'utilisateur √† la file d'attente
        _users_batch_queue.append({
            "user_id": user_id,
            "username": username,
            "timestamp": time.time(),
            "referrer_id": referrer_id
        })
        
        # Si la file atteint 20 utilisateurs ou si 30 secondes se sont √©coul√©es,
        # d√©clencher le traitement par lots
        batch_size_threshold = 20  # Augment√© pour r√©duire les requ√™tes
        batch_time_threshold = 30  # secondes
        
        if (len(_users_batch_queue) >= batch_size_threshold or 
                (time.time() - _last_batch_processing) > batch_time_threshold):
            # Traiter le lot en arri√®re-plan
            asyncio.create_task(process_users_batch())
        
        return True
    except Exception as e:
        logger.error(f"Erreur lors de l'ajout de l'utilisateur √† la file d'attente: {e}")
        return False

# Fonctions d'acc√®s aux donn√©es des matchs (avec cache)
def get_all_matches_data():
    """
    R√©cup√®re les donn√©es des matchs depuis la base de donn√©es active.
    Version optimis√©e qui utilise le cache.
    """
    # Utiliser asyncio pour permettre l'utilisation du cache
    async def get_matches_async():
        # V√©rifier d'abord le cache
        cached_matches = await get_cached_matches()
        if cached_matches:
            logger.info("Utilisation du cache pour les donn√©es de matchs")
            return cached_matches
        
        # Si pas en cache, charger depuis la base de donn√©es
        try:
            matches = db.get_all_matches_data()
            # Mettre en cache pour les requ√™tes futures (24h)
            await cache_matches(matches)
            return matches
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des matchs: {e}")
            return []
    
    # Ex√©cuter la fonction asynchrone de mani√®re synchrone
    loop = asyncio.get_event_loop()
    try:
        return loop.run_until_complete(get_matches_async())
    except RuntimeError:
        # Si aucune boucle d'√©v√©nements n'est disponible, en cr√©er une nouvelle
        new_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(new_loop)
        try:
            return new_loop.run_until_complete(get_matches_async())
        finally:
            new_loop.close()

def get_all_teams():
    """
    R√©cup√®re la liste de toutes les √©quipes.
    Version optimis√©e qui utilise le cache.
    """
    # Utiliser asyncio pour permettre l'utilisation du cache
    async def get_teams_async():
        # V√©rifier d'abord le cache
        cached_teams = await get_cached_teams()
        if cached_teams:
            logger.info("Utilisation du cache pour la liste des √©quipes")
            return cached_teams
        
        # Si pas en cache, charger depuis la base de donn√©es
        try:
            teams = db.get_all_teams()
            # Mettre en cache pour les requ√™tes futures (24h)
            await cache_teams(teams)
            return teams
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des √©quipes: {e}")
            return []
    
    # Ex√©cuter la fonction asynchrone de mani√®re synchrone
    loop = asyncio.get_event_loop()
    try:
        return loop.run_until_complete(get_teams_async())
    except RuntimeError:
        # Si aucune boucle d'√©v√©nements n'est disponible, en cr√©er une nouvelle
        new_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(new_loop)
        try:
            return new_loop.run_until_complete(get_teams_async())
        finally:
            new_loop.close()

def get_team_statistics(matches):
    """Calcule les statistiques pour chaque √©quipe"""
    try:
        return db.get_team_statistics(matches)
    except Exception as e:
        logger.error(f"Erreur lors du calcul des statistiques d'√©quipe: {e}")
        return {}

def get_match_id_trends(matches):
    """Analyse les tendances par num√©ro de match"""
    try:
        return db.get_match_id_trends(matches)
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse des tendances par match: {e}")
        return {}

def get_common_scores(scores_list, top_n=5):
    """Retourne les scores les plus communs avec leur fr√©quence"""
    try:
        return db.get_common_scores(scores_list, top_n)
    except Exception as e:
        logger.error(f"Erreur lors du calcul des scores communs: {e}")
        return []

def get_direct_confrontations(matches, team1, team2):
    """R√©cup√®re l'historique des confrontations directes entre deux √©quipes"""
    try:
        return db.get_direct_confrontations(matches, team1, team2)
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des confrontations directes: {e}")
        return []

def save_prediction_log(user_id, username, team1, team2, odds1=None, odds2=None, prediction_result=None):
    """
    Enregistre les pr√©dictions demand√©es par les utilisateurs.
    Version optimis√©e qui ajoute la t√¢che √† la file d'attente de basse priorit√©.
    """
    async def _save_prediction_log_async():
        try:
            return db.save_prediction_log(user_id, username, team1, team2, odds1, odds2, prediction_result)
        except Exception as e:
            logger.error(f"Erreur lors de l'enregistrement de la pr√©diction: {e}")
            return False
    
    # Ajouter √† la file d'attente avec basse priorit√© (pas critique)
    asyncio.create_task(queue_manager.add_low_priority(_save_prediction_log_async))
    return True

async def check_user_subscription(user_id):
    """
    V√©rifie si un utilisateur est abonn√© au canal.
    Version optimis√©e qui utilise le cache pour r√©duire drastiquement les requ√™tes API.
    
    Args:
        user_id (int): L'ID de l'utilisateur Telegram √† v√©rifier
        
    Returns:
        bool: True si l'utilisateur est abonn√©, False sinon
    """
    # Convertir en string pour la coh√©rence
    user_id_str = str(user_id)
    
    # V√©rifier si c'est un admin (toujours en direct, pas de cache)
    try:
        from admin_access import is_admin
        if is_admin(user_id):
            logger.info(f"V√©rification d'abonnement contourn√©e pour l'admin (ID: {user_id})")
            return True
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification du statut admin: {e}")
    
    # V√©rifier le cache
    cached_status = await get_cached_subscription_status(user_id)
    if cached_status is not None:
        logger.info(f"Utilisation du cache pour la v√©rification d'abonnement de l'utilisateur {user_id}")
        return cached_status
    
    # V√©rification via la base de donn√©es active
    try:
        is_subscribed = await db.check_user_subscription(user_id)
        # Mettre en cache (24h)
        await cache_subscription_status(user_id, is_subscribed)
        return is_subscribed
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification d'abonnement: {e}")
        # En cas d'erreur, supposer que l'utilisateur est abonn√© pour √©viter le blocage
        # Cette logique peut √™tre modifi√©e selon votre politique
        return True

# Fonctions pour le syst√®me de parrainage avec mise en cache
async def register_user(user_id, username, referrer_id=None):
    """
    Enregistre ou met √† jour un utilisateur dans la base de donn√©es.
    Version optimis√©e qui utilise la file d'attente par lots.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        username (str): Nom d'utilisateur Telegram
        referrer_id (int, optional): ID Telegram du parrain
    
    Returns:
        bool: True si l'op√©ration a r√©ussi, False sinon
    """
    # V√©rifier si c'est un admin
    try:
        from admin_access import is_admin
        if is_admin(user_id, username):
            logger.info(f"Enregistrement d'un administrateur: {username} (ID: {user_id})")
            return True
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification du statut admin: {e}")
    
    # Ajouter l'utilisateur √† la file d'attente pour traitement par lots
    success = await add_user_to_batch_queue(user_id, username, referrer_id)
    return success

async def create_referral_relationship(user_id, referrer_id):
    """
    Cr√©e une relation de parrainage dans la base de donn√©es.
    Version optimis√©e qui utilise la file d'attente pour les op√©rations non-critiques.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur parrain√©
        referrer_id (int): ID Telegram du parrain
    """
    # V√©rifier si c'est un admin
    try:
        from admin_access import is_admin
        if is_admin(user_id) or is_admin(referrer_id):
            logger.info(f"Relation de parrainage impliquant un admin. ID Utilisateur: {user_id}, ID Parrain: {referrer_id}")
            return None
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification du statut admin: {e}")
    
    # Cr√©er la relation via la base de donn√©es active
    async def _create_referral_async():
        try:
            return await db.create_referral_relationship(user_id, referrer_id)
        except Exception as e:
            logger.error(f"Erreur lors de la cr√©ation de la relation de parrainage: {e}")
            return None
    
    # Ajouter √† la file d'attente avec priorit√© moyenne
    return await queue_manager.add_medium_priority(_create_referral_async)

async def verify_and_update_referral(user_id, referrer_id):
    """
    V√©rifie si l'utilisateur est abonn√© au canal et met √† jour le statut de parrainage.
    Version optimis√©e qui utilise la file d'attente pour les op√©rations non-critiques.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        referrer_id (int): ID Telegram du parrain
    """
    # Attendre moins longtemps (2 secondes au lieu de 30)
    await asyncio.sleep(2)
    
    # V√©rifier l'abonnement
    is_subscribed = await check_user_subscription(user_id)
    logger.info(f"V√©rification d'abonnement pour user {user_id}: {is_subscribed}")
    
    if is_subscribed:
        # Mettre √† jour le statut de v√©rification
        async def _update_referral_async():
            try:
                db_instance = get_database()
                if db_instance is None:
                    logger.error("Impossible de se connecter √† la base de donn√©es pour v√©rifier un parrainage")
                    return None
                
                # Mettre √† jour le statut de v√©rification
                current_time = datetime.now().isoformat()
                result = await db_instance.referrals.update_one(
                    {
                        "referrer_id": str(referrer_id),
                        "referred_id": str(user_id)
                    },
                    {
                        "$set": {
                            "verified": True,
                            "verification_date": current_time
                        }
                    }
                )
                
                if result.modified_count > 0:
                    logger.info(f"Parrainage v√©rifi√©: {referrer_id} -> {user_id}")
                    
                    # Invalider le cache pour ces utilisateurs
                    await cache_referral_count(referrer_id, None)  # Forcer un rechargement
                    
                    # Notification au parrain
                    try:
                        from telegram import Bot
                        from config import TELEGRAM_TOKEN
                        
                        bot = Bot(token=TELEGRAM_TOKEN)
                        referral_count = await count_referrals(referrer_id)
                        
                        from referral_system import get_max_referrals
                        max_referrals = await get_max_referrals()
                        
                        await bot.send_message(
                            chat_id=referrer_id,
                            text=f"üéâ *F√©licitations!* Un nouvel utilisateur a utilis√© votre lien et s'est abonn√© au canal.\n\n"
                                 f"Vous avez maintenant *{referral_count}/{max_referrals}* parrainages v√©rifi√©s.",
                            parse_mode='Markdown'
                        )
                    except Exception as e:
                        logger.error(f"Erreur lors de l'envoi de la notification au parrain: {e}")
                    
                    return True
                else:
                    logger.warning(f"Aucune mise √† jour du statut de parrainage pour {referrer_id} -> {user_id}")
                    return False
            except Exception as e:
                logger.error(f"Erreur lors de la v√©rification du parrainage: {e}")
                return None
        
        # Ajouter √† la file d'attente avec priorit√© moyenne
        return await queue_manager.add_medium_priority(_update_referral_async)
    else:
        logger.info(f"Utilisateur {user_id} non abonn√©, parrainage non v√©rifi√©")
        return False

async def has_completed_referrals(user_id, username=None):
    """
    V√©rifie si l'utilisateur a atteint le nombre requis de parrainages.
    Version optimis√©e qui utilise le cache.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        username (str, optional): Nom d'utilisateur Telegram pour v√©rification admin
        
    Returns:
        bool: True si l'utilisateur a compl√©t√© ses parrainages ou est admin, False sinon
    """
    # V√©rifier si c'est un admin
    try:
        from admin_access import is_admin
        if is_admin(user_id, username):
            logger.info(f"V√©rification de parrainage contourn√©e pour l'admin {username} (ID: {user_id})")
            return True
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification du statut admin: {e}")
    
    # Obtenir le nombre de parrainages (via cache)
    referral_count = await count_referrals(user_id)
    
    from referral_system import get_max_referrals
    max_referrals = await get_max_referrals()
    
    # V√©rifier si le quota est atteint
    completed = referral_count >= max_referrals
    logger.info(f"Utilisateur {user_id} a {referral_count}/{max_referrals} parrainages - Statut: {'Compl√©t√©' if completed else 'En cours'}")
    
    return completed

async def count_referrals(user_id):
    """
    Compte le nombre de parrainages v√©rifi√©s pour un utilisateur.
    Version optimis√©e qui utilise le cache.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        
    Returns:
        int: Le nombre de parrainages v√©rifi√©s
    """
    # V√©rifier si c'est un admin
    try:
        from admin_access import is_admin
        if is_admin(user_id):
            logger.info(f"Comptage de parrainage contourn√© pour l'admin (ID: {user_id})")
            
            from referral_system import get_max_referrals
            max_referrals = await get_max_referrals()
            return max_referrals
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification du statut admin: {e}")
    
    # V√©rifier le cache
    cached_count = await get_cached_referral_count(user_id)
    if cached_count is not None:
        logger.info(f"Utilisation du cache pour le comptage des parrainages de l'utilisateur {user_id}")
        return cached_count
    
    # Si pas en cache, r√©cup√©rer depuis la base de donn√©es
    try:
        count = await db.count_referrals(user_id)
        
        # Mettre en cache
        await cache_referral_count(user_id, count)
        
        return count
    except Exception as e:
        logger.error(f"Erreur lors du comptage des parrainages: {e}")
        return 0

# Version l√©g√®re pour la v√©rification rapide des parrainages
async def count_referrals_lite(user_id):
    """
    Version all√©g√©e du comptage de parrainages pour les v√©rifications fr√©quentes.
    Utilise uniquement le cache, sans requ√™te √† la base de donn√©es.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        
    Returns:
        int: Le nombre de parrainages v√©rifi√©s ou 0 si non trouv√© dans le cache
    """
    # V√©rifier si c'est un admin
    try:
        from admin_access import is_admin
        if is_admin(user_id):
            from referral_system import get_max_referrals
            max_referrals = await get_max_referrals()
            return max_referrals
    except Exception:
        pass
    
    # V√©rifier uniquement le cache
    cached_count = await get_cached_referral_count(user_id)
    if cached_count is not None:
        return cached_count
    
    # Si pas en cache, retourner 0 (forcer une v√©rification compl√®te)
    return 0

async def get_referred_users(user_id):
    """
    R√©cup√®re la liste des utilisateurs parrain√©s par un utilisateur.
    Version optimis√©e qui utilise la file d'attente pour les requ√™tes non-critiques.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        
    Returns:
        list: Liste des utilisateurs parrain√©s avec leurs informations
    """
    # V√©rifier si c'est un admin
    try:
        from admin_access import is_admin
        if is_admin(user_id):
            logger.info(f"R√©cup√©ration des parrainages contourn√©e pour l'admin (ID: {user_id})")
            return []
    except Exception as e:
        logger.error(f"Erreur lors de la v√©rification du statut admin: {e}")
    
    # R√©cup√©rer via la base de donn√©es active
    async def _get_referred_users_async():
        try:
            return await db.get_referred_users(user_id)
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des utilisateurs parrain√©s: {e}")
            return []
    
    # Ajouter √† la file d'attente avec priorit√© moyenne
    return await queue_manager.add_medium_priority(_get_referred_users_async)

async def generate_referral_link(user_id, bot_username):
    """
    G√©n√®re un lien de parrainage pour un utilisateur.
    
    Args:
        user_id (int): ID Telegram de l'utilisateur
        bot_username (str): Nom d'utilisateur du bot
        
    Returns:
        str: Lien de parrainage
    """
    try:
        return await db.generate_referral_link(user_id, bot_username)
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du lien de parrainage: {e}")
        # Fallback au cas o√π la fonction de la base de donn√©es √©choue
        return f"https://t.me/{bot_username}?start=ref{user_id}"

async def get_max_referrals():
    """
    R√©cup√®re le nombre maximum de parrainages requis.
    
    Returns:
        int: Nombre maximum de parrainages requis
    """
    try:
        from config import MAX_REFERRALS
        return MAX_REFERRALS
    except ImportError:
        try:
            return await db.get_max_referrals()
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration du nombre max de parrainages: {e}")
            return 1  # Valeur par d√©faut

def get_referral_instructions():
    """
    Retourne les instructions pour qu'un parrainage soit valid√©.
    
    Returns:
        str: Message format√© avec les instructions
    """
    try:
        return db.get_referral_instructions()
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des instructions de parrainage: {e}")
        # Texte par d√©faut si la fonction √©choue
        return (
            "*üìã Conditions pour qu'un parrainage soit valid√©:*\n\n"
            "1Ô∏è‚É£ *L'invit√© doit cliquer sur votre lien de parrainage*\n"
            "2Ô∏è‚É£ *L'invit√© doit d√©marrer le bot* avec la commande /start\n"
            "3Ô∏è‚É£ *L'invit√© doit s'abonner* au canal [AL VE CAPITAL](https://t.me/alvecapitalofficiel)\n\n"
            "_Note: Le parrainage sera automatiquement v√©rifi√© et valid√© une fois ces conditions remplies_"
        )

# Fonctions d'optimisation des performances

async def preload_static_data():
    """
    Pr√©charge les donn√©es statiques (√©quipes, matchs) au d√©marrage pour am√©liorer les performances.
    Cette fonction doit √™tre appel√©e au d√©marrage du bot.
    """
    logger.info("Pr√©chargement des donn√©es statiques...")
    
    try:
        # Pr√©charger les matchs
        matches = db.get_all_matches_data()
        if matches:
            # Mettre en cache
            await cache_matches(matches)
            logger.info(f"Pr√©chargement de {len(matches)} matchs termin√©")
            
            # Pr√©charger les √©quipes
            teams = []
            team_set = set()
            
            for match in matches:
                team_home = match.get('team_home', '')
                team_away = match.get('team_away', '')
                
                if team_home and team_home not in team_set:
                    team_set.add(team_home)
                    teams.append(team_home)
                
                if team_away and team_away not in team_set:
                    team_set.add(team_away)
                    teams.append(team_away)
            
            # Trier les √©quipes
            teams.sort()
            
            # Mettre en cache
            await cache_teams(teams)
            logger.info(f"Pr√©chargement de {len(teams)} √©quipes termin√©")
        else:
            logger.warning("Aucun match trouv√© pour le pr√©chargement")
            
            # Essayer de charger directement les √©quipes
            teams = db.get_all_teams()
            if teams:
                await cache_teams(teams)
                logger.info(f"Pr√©chargement de {len(teams)} √©quipes termin√©")
            else:
                logger.warning("Aucune √©quipe trouv√©e pour le pr√©chargement")
        
        return True
    except Exception as e:
        logger.error(f"Erreur lors du pr√©chargement des donn√©es statiques: {e}")
        return False

async def monitor_database_performance():
    """
    Surveille les performances de la base de donn√©es et journalise les statistiques.
    Cette fonction doit √™tre d√©marr√©e en arri√®re-plan au d√©marrage du bot.
    """
    try:
        # M√©triques de performance
        metrics = {
            "operations": 0,
            "successes": 0,
            "failures": 0,
            "response_times": [],
            "batch_operations": 0,
            "avg_response_time": 0
        }
        
        # Boucle de surveillance
        while True:
            # Attendre 15 minutes
            await asyncio.sleep(900)
            
            # Journaliser les statistiques
            total_operations = metrics["operations"]
            if total_operations > 0:
                success_rate = (metrics["successes"] / total_operations) * 100
                logger.info(f"Statistiques de base de donn√©es - Op√©rations: {total_operations}, "
                           f"Succ√®s: {metrics['successes']} ({success_rate:.1f}%), "
                           f"√âchecs: {metrics['failures']}, "
                           f"Lots: {metrics['batch_operations']}")
                
                if metrics["response_times"]:
                    avg_time = sum(metrics["response_times"]) / len(metrics["response_times"])
                    logger.info(f"Temps de r√©ponse moyen: {avg_time:.3f}s")
                
                # R√©initialiser les compteurs
                metrics["operations"] = 0
                metrics["successes"] = 0
                metrics["failures"] = 0
                metrics["response_times"] = []
                metrics["batch_operations"] = 0
    except asyncio.CancelledError:
        logger.info("Surveillance de la base de donn√©es arr√™t√©e")
    except Exception as e:
        logger.error(f"Erreur lors de la surveillance de la base de donn√©es: {e}")

async def initialize_database():
    """
    Initialise la base de donn√©es et pr√©charge les donn√©es.
    Cette fonction doit √™tre appel√©e au d√©marrage de l'application.
    """
    logger.info("Initialisation de la base de donn√©es...")
    
    try:
        # Pr√©charger les donn√©es statiques
        await preload_static_data()
        
        # D√©marrer la surveillance des performances en arri√®re-plan
        asyncio.create_task(monitor_database_performance())
        
        # Cr√©er les index MongoDB si n√©cessaire
        if USE_MONGODB:
            db_instance = get_database()
            if db_instance:
                # Index pour les matchs
                db_instance.matches.create_index("match_id")
                db_instance.matches.create_index("team_home")
                db_instance.matches.create_index("team_away")
                
                # Index pour les utilisateurs
                db_instance.users.create_index("user_id", unique=True)
                db_instance.users.create_index("username")
                db_instance.users.create_index("referred_by")
                
                # Index pour les parrainages
                db_instance.referrals.create_index([("referrer_id", 1), ("referred_id", 1)], unique=True)
                db_instance.referrals.create_index("verified")
                
                # Index pour les logs de pr√©dictions
                db_instance.prediction_logs.create_index("user_id")
                db_instance.prediction_logs.create_index("date")
                
                logger.info("Cr√©ation des index MongoDB termin√©e avec succ√®s")
        
        logger.info("Initialisation de la base de donn√©es termin√©e avec succ√®s")
        return True
    except Exception as e:
        logger.error(f"Erreur lors de l'initialisation de la base de donn√©es: {e}")
        return False

# Fonctions pour les tests et l'administration

async def get_database_status():
    """
    R√©cup√®re le statut de la base de donn√©es pour le monitoring.
    Utile pour les commandes administratives.
    
    Returns:
        dict: Statistiques de la base de donn√©es
    """
    try:
        db_status = {
            "type": "MongoDB" if USE_MONGODB else "Google Sheets",
            "active": False,
            "collections": {},
            "cache_hit_rate": 0,
            "last_batch_size": len(_users_batch_queue),
            "time_since_last_batch": round(time.time() - _last_batch_processing)
        }
        
        if USE_MONGODB:
            try:
                db_instance = get_database()
                if db_instance:
                    # V√©rifier la connexion
                    db_instance.command("ping")
                    db_status["active"] = True
                    
                    # Compter les documents dans chaque collection
                    for collection_name in ["matches", "users", "referrals", "prediction_logs"]:
                        collection = getattr(db_instance, collection_name)
                        db_status["collections"][collection_name] = collection.count_documents({})
            except Exception as e:
                logger.error(f"Erreur lors de la v√©rification de MongoDB: {e}")
                db_status["error"] = str(e)
        else:
            # Pour Google Sheets, simplement marquer comme actif
            db_status["active"] = True
        
        # Statistiques de cache
        from cache_system import cache
        cache_stats = cache.get_stats()
        db_status["cache_stats"] = cache_stats
        
        if cache_stats.get("hits", 0) + cache_stats.get("misses", 0) > 0:
            total = cache_stats.get("hits", 0) + cache_stats.get("misses", 0)
            db_status["cache_hit_rate"] = round((cache_stats.get("hits", 0) / total) * 100, 1)
        
        return db_status
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration du statut de la base de donn√©es: {e}")
        return {"error": str(e), "active": False}

async def admin_force_preload():
    """
    Force le pr√©chargement des donn√©es statiques.
    Utile pour les commandes administratives.
    
    Returns:
        bool: True si l'op√©ration a r√©ussi
    """
    try:
        # Vider les caches existants
        from cache_system import cache
        await cache.clear_all()
        
        # Pr√©charger les donn√©es
        success = await preload_static_data()
        
        return success
    except Exception as e:
        logger.error(f"Erreur lors du pr√©chargement forc√©: {e}")
        return False

async def admin_clear_cache():
    """
    Vide enti√®rement le cache.
    Utile pour les commandes administratives.
    
    Returns:
        bool: True si l'op√©ration a r√©ussi
    """
    try:
        from cache_system import cache
        await cache.clear_all()
        logger.info("Cache vid√© avec succ√®s")
        return True
    except Exception as e:
        logger.error(f"Erreur lors du vidage du cache: {e}")
        return False
